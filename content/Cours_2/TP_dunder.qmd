---
title: "TP: Heritage avec le pricing d'option"
weight: 20
tags:
 - POO
 - Dunder
 - Composition
 - Tensors
 - AD
categories:
 - TP
 - Avanc√©
type: book
image: .img/tpfinance.png
filters:
 - pyodide
---

## Understanding Tensors and Automatic Differentiation

### The Power of Composition

Imagine you have a complex mathematical function. It might look intimidating at first, but it's actually just a composition of simple operations. For example, consider this function:

$f(x, y) = (x^2 + 2y) * \sin(x + y)$

We can break this down into simpler operations:

1. $a = x^2$
2. $b = 2y$
3. $c = a + b$ (which is $x^2 + 2y$)
4. $d = x + y$
5. $e = \sin(d)$ (which is $\sin(x + y)$)
6. $f = c * e$ (our final result)

### Automatic Differentiation: It's All About the Chain Rule

Now, here's the magic: if we know how to differentiate each of these simple operations, we can automatically compute the derivative of the entire complex function. This is the essence of automatic differentiation.

Let's say we want to find $\frac{\partial f}{\partial x}$. We can use the chain rule:

$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial c} \cdot \frac{\partial c}{\partial x} + \frac{\partial f}{\partial e} \cdot \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial x}$

Breaking it down:
- $\frac{\partial f}{\partial c} = e$
- $\frac{\partial c}{\partial x} = 2x$
- $\frac{\partial f}{\partial e} = c$
- $\frac{\partial e}{\partial d} = \cos(d)$
- $\frac{\partial d}{\partial x} = 1$

Putting it all together:

$\frac{\partial f}{\partial x} = e \cdot 2x + c \cdot \cos(d) \cdot 1$

### Tensors: Tracking Operations and Their Derivatives

This is where tensors come in. In our implementation, a tensor will not only store its value but also remember:
1. The operation that created it
2. The tensors that were inputs to this operation
3. How to compute its derivative with respect to its inputs

By doing this for each operation, we create a computational graph. When we want to compute the derivative of our final result with respect to any input, we can simply walk backwards through this graph, applying the chain rule at each step.

## TP Instructions: Implementing a Basic Tensor Class with Automatic Differentiation

Your task is to implement a simplified Tensor class that supports basic mathematical operations and automatic differentiation. This class will allow us to build simple computational graphs and compute gradients automatically.

Here's a skeleton of the Tensor class to get you started:

```{pyodide-python}
import numpy as np

class Tensor:
    def __init__(self, data, _children=(), _op=''):
        self.data = np.array(data).astype(np.float64)
        self.grad = np.zeros_like(self.data)
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op

    def __add__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(self.data + other.data, (self, other), '+')

        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward

        return out

    def __mul__(self, other):
        other = other if isinstance(other, Tensor) else Tensor(other)
        out = Tensor(self.data * other.data, (self, other), '*')

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward

        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float)), "only supporting int/float powers for now"
        out = Tensor(self.data**other, (self,), f'**{other}')

        def _backward():
            self.grad += (other * self.data**(other-1)) * out.grad
        out._backward = _backward

        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)

        self.grad = np.ones_like(self.data)
        for v in reversed(topo):
            v._backward()

def tensor(data):
    return Tensor(data)
```

Your tasks:

1. Implement the reversed methods for already existing one like `__radd__` or `__rmul__`.
2. Implement the `__sub__` and `__truediv__` methods for subtraction and division operations.
3. Add support for operations between Tensors and regular numbers (scalars) in all methods.
4. Implement a `sin()` method that computes the sine of a Tensor.
5. Add proper string representation methods (`__repr__` and `__str__`).

## Example Usage: Computing Gradients of a Complex Function

After implementing the Tensor class, you can use it to compute gradients of complex functions. Here's an example using the function we discussed earlier:

```{pyodide-python}
def f(x, y):
    return (x**2 + 2*y) * Tensor.sin(x + y)

# Create input tensors
x = tensor(1.0)
y = tensor(2.0)

# Compute the function
result = f(x, y)

# Compute gradients
result.backward()

print(f"f({x.data}, {y.data}) = {result.data}")
print(f"df/dx = {x.grad}")
print(f"df/dy = {y.grad}")
```

This example demonstrates how your Tensor class can be used to automatically compute gradients of a complex function. The `backward()` method computes the gradients with respect to all input tensors.

## Unitary Tests

Here are some unitary tests to verify your implementation. Implement your Tensor class in a file named `tensor.py`, and then use these tests to check your work:

```{pyodide-python}
import unittest
import numpy as np

class TestTensor(unittest.TestCase):
    def setUp(self):
        self.x = tensor(2.0)
        self.y = tensor(3.0)

    def test_add(self):
        z = self.x + self.y
        z.backward()
        self.assertEqual(z.data, 5.0)
        self.assertEqual(self.x.grad, 1.0)
        self.assertEqual(self.y.grad, 1.0)

    def test_mul(self):
        z = self.x * self.y
        z.backward()
        self.assertEqual(z.data, 6.0)
        self.assertEqual(self.x.grad, 3.0)
        self.assertEqual(self.y.grad, 2.0)

    def test_pow(self):
        z = self.x ** 2
        z.backward()
        self.assertEqual(z.data, 4.0)
        self.assertEqual(self.x.grad, 4.0)

    def test_sub(self):
        z = self.x - self.y
        z.backward()
        self.assertEqual(z.data, -1.0)
        self.assertEqual(self.x.grad, 1.0)
        self.assertEqual(self.y.grad, -1.0)

    def test_div(self):
        z = self.x / self.y
        z.backward()
        self.assertAlmostEqual(z.data, 2/3, places=5)
        self.assertAlmostEqual(self.x.grad, 1/3, places=5)
        self.assertAlmostEqual(self.y.grad, -2/9, places=5)

    def test_sin(self):
        z = Tensor.sin(self.x)
        z.backward()
        self.assertAlmostEqual(z.data, np.sin(2.0), places=5)
        self.assertAlmostEqual(self.x.grad, np.cos(2.0), places=5)

    def test_complex_function(self):
        def f(x, y):
            return (x**2 + 2*y) * Tensor.sin(x + y)
        
        x = tensor(1.0)
        y = tensor(2.0)
        z = f(x, y)
        z.backward()
        
        expected_z = (1**2 + 2*2) * np.sin(1 + 2)
        expected_dx = 2 * np.sin(3) + 5 * np.cos(3)
        expected_dy = 2 * np.sin(3) + 5 * np.cos(3)
        
        self.assertAlmostEqual(z.data, expected_z, places=5)
        self.assertAlmostEqual(x.grad, expected_dx, places=5)
        self.assertAlmostEqual(y.grad, expected_dy, places=5)

def run_tests():
    unittest.main(argv=[''], verbosity=2, exit=False)

if __name__ == '__main__':
    run_tests()
```
